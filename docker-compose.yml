# Notes :
# Basé à partir de la doc Airflow : https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html
#
# Lancer le fichier "scripts/docker_compose_down_up.sh" qui s'occupe de réinitialiser la conf docker (voir le script).

x-airflow-common:
  &airflow-common
  # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.0.2}
  # build: .   # build à partir du Dockerfile
  build:
    context: .
    dockerfile: airflow/Dockerfile
  environment:
    &airflow-common-env
    AIRFLOW_CONFIG: /opt/airflow/config/airflow.cfg
    # les paramètres suivants écrasent ceux qui sont dans le "airflow.cfg" :
    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://mhh:mhh@postgres/francetravail
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://mhh:mhh@postgres/francetravail
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: m0XspG5J7rDL3xRpMM5k0LzC6M6x73dOPnbrjX3_cug=  # indispensable, car sinon ie on verra dans la page "Connections" l'erreur "500 Internal Server Error"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: True
    AIRFLOW__CORE__LOAD_EXAMPLES: False
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: http://airflow-apiserver:8080/execution/
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: True
    AIRFLOW__METRICS__STATSD_ON: True  # active la collecte des métriques par StatsD destinées au statsd-exporter
    AIRFLOW__METRICS__STATSD_HOST: statsd-exporter
    AIRFLOW__METRICS__STATSD_PORT: 9125
    AIRFLOW__METRICS__STATSD_PREFIX: airflow

  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/data:/opt/airflow/data
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/config:/opt/airflow/config
    # - ./airflow/plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy


services:

  ##################################

  postgres:
    image: postgres:16-alpine
    container_name: postgres
    environment:
      POSTGRES_DB: francetravail
      POSTGRES_USER: mhh
      POSTGRES_PASSWORD: mhh
    ports:
      - "5432:5432"
    volumes:
      - postgres_data_job_market:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mhh -d francetravail"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  ##################################

  fastapi:
    build:
      context: .  # contexte = dossier "Job_Market" car fichier "code_name__city_department_region.csv" en dehors du dossier "fastapi"
      dockerfile: fastapi/Dockerfile
    container_name: fastapi
    ports:
      - "8000:8000"
    environment:
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=francetravail
      - DB_USER=mhh
      - DB_PASSWORD=mhh
    volumes:
      # utile en mode dev (montage de volumes pour ne pas avoir à relancer docker-compose après chaque modif)
      - ./fastapi:/app  # pour avoir le "main.py" et tous les fichiers pgsql (que je modifie régulièrement en mode dev)
      - ./airflow/data/resources:/app/locations_information
    depends_on:
      - postgres

  ##################################

  redis:
    image: redis
    container_name: redis
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always

  ##################################

  airflow-apiserver:
    container_name: airflow-apiserver
    <<: *airflow-common
    command: api-server
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/api/v2/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    # depends_on:
    #   <<: *airflow-common-depends-on
    #   airflow-init:
    #     condition: service_completed_successfully

  ##################################

  airflow-scheduler:
    container_name: airflow-scheduler
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    # depends_on:
    #   <<: *airflow-common-depends-on
    #   airflow-init:
    #     condition: service_completed_successfully

  ##################################

  airflow-dag-processor:
    container_name: airflow-dag-processor
    <<: *airflow-common
    command: dag-processor
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    # depends_on:
    #   <<: *airflow-common-depends-on
    #   airflow-init:
    #     condition: service_completed_successfully

  ##################################

  airflow-worker:
    container_name: airflow-worker
    <<: *airflow-common
    command: celery worker
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      DUMB_INIT_SETSID: "0"
    restart: always
    # depends_on:
    #   <<: *airflow-common-depends-on
    #   airflow-apiserver:
    #     condition: service_healthy
    #   airflow-init:
    #     condition: service_completed_successfully

  ##################################

  airflow-triggerer:
    container_name: airflow-triggerer
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    # depends_on:
    #   <<: *airflow-common-depends-on
    #   airflow-init:
    #     condition: service_completed_successfully

  ##################################

  airflow-init:
    container_name: airflow-init
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
          export AIRFLOW_UID=$(id -u)
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        echo
        echo "Creating missing opt dirs if missing:"
        echo
        mkdir -v -p /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Airflow version:"
        /entrypoint airflow version
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Running airflow config list to create default config file if missing."
        echo
        /entrypoint airflow config list >/dev/null
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Change ownership of files in /opt/airflow to ${AIRFLOW_UID}:0"
        echo
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/
        echo
        echo "Change ownership of files in shared volumes to ${AIRFLOW_UID}:0"
        echo
        chown -v -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}

    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"

  ##################################

  airflow-cli:
    container_name: airflow-cli
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command:
      - bash
      - -c
      - airflow
    # depends_on:
    #   <<: *airflow-common-depends-on

  ##################################

  flower:
    container_name: flower
    <<: *airflow-common
    command: celery flower
    profiles:
      - flower
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    # depends_on:
    #   <<: *airflow-common-depends-on
    #   airflow-init:
    #     condition: service_completed_successfully


  ##################################

  # https://github.com/statsd/statsd
  #  pour exporter les métriques de StatsD vers Prometheus

  statsd-exporter:
    image: prom/statsd-exporter
    container_name: statsd-exporter
    volumes:
      - ./airflow/config/statsd.yaml:/home/statsd-mapping-configs.yaml
    entrypoint: ["/bin/sh", "-c", "--"]
    command: ["statsd_exporter --log.level debug --statsd.mapping-config=/home/statsd-mapping-configs.yaml"]
    ports:
      - 9102:9102  # scrape port (pour Prometheus)
      - 9125:9125  # ingest port (pour recevoir les métriques de StatsD)
    restart: always

  ##################################

  node-exporter:
    image: prom/node-exporter
    container_name: node-exporter
    ports:
      - 9100:9100  # scrape port (pour Prometheus)
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/host:ro
    command:
      - '--path.rootfs=/host'
    restart: always

  ##################################

  postgres-exporter:
    image: quay.io/prometheuscommunity/postgres-exporter
    container_name: postgres-exporter
    environment:
      DATA_SOURCE_NAME: "postgresql://mhh:mhh@postgres:5432/francetravail?sslmode=disable"
    ports:
      - 9187:9187  # scrape port (pour Prometheus)
    depends_on:
      - postgres
    restart: always

  ##################################

  prometheus:
    image: prom/prometheus
    container_name: prometheus
    volumes:
      - ./airflow/config/prometheus.yaml:/etc/prometheus/prometheus.yaml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yaml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    ports:
      - 9092:9090
    restart: always

  ##################################

  grafana:
    image: grafana/grafana
    container_name: grafana
    environment:
      # https://grafana.com/docs/grafana/latest/setup-grafana/configure-docker/#default-paths
      GF_SECURITY_ADMIN_USER: grafana
      GF_SECURITY_ADMIN_PASSWORD: grafana
      GF_PATHS_PROVISIONING: /grafana/provisioning  # default : /etc/grafana/provisioning
    ports:
      - 3000:3000
    volumes:
      # https://grafana.com/docs/grafana/latest/administration/provisioning/#data-sources :
      #   "You can manage data sources in Grafana by adding YAML configuration files in the provisioning/datasources directory"
      - ./grafana/volumes/provisioning:/grafana/provisioning
    restart: always

  ##################################

volumes:
  postgres_data_job_market:
  prometheus_data:
  grafana_data: